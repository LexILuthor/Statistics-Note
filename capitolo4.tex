\chapter{Statistics}
\vspace{15pt}



The notation of statistic was introduced by fisher (1920).\\
The importance of sufficiency is that ot can be found in any statistical decision (point estimation, testing, confidential bound)
\begin{defi}
	Let $X=(X_1... X_n)$ be a random sample from a parametric model $X\sim f_X(x,\theta)$ for some $\theta \in \Theta$ unknown.\\
	We say that $T_n=T(X)$ \textbf{sufficient for the parameter $\theta$} if the conditional distribution of $X$ given $T_n$ does not depend of $\theta$ i.e.
	\begin{itemize}
		\item $f_{\uX|T_n=t}(X;t,\theta)$ conditional distribution of $\uX$ given $T_n$
		\item $h_{\uX,T_n}(z,t,\theta)$ joint distribution of $\uX$ and $T_n$
		\item $g_{T_n}(t,\theta)$ marginal distribution of $T_n$
	\end{itemize}
then $T_n$ is \textbf{sufficient for the parameter $\theta$} only if $f_{\uX|T_n=t}(X;t,\theta)$ does not depend of $\theta$.\\
Note that
\[
f_{\uX|T_n=t}(X;t,\theta)=\frac{h_{\uX,T_n}(z,t,\theta)}{g_{T_n}(t,\theta)}
\]
\end{defi}
\begin{eg} \label{eg:ber}
	$(X_1... X_n)\in \{0,1\}^n$ from a $Ber(\theta)$, $\theta \in (0,1)$.\\
	Define $T_n=\sum_{i=1}^{n}X_i$ then we want to verify if $T_n$ is sufficient.\\
	\begin{itemize}
		\item $f_{\uX}(\ux;\theta) = \prod_{i=1}^{n} f_{\underline{X_i}}(\underline{x_i};\theta)=\theta^{\sum_{i=1}^{n}x_i}(1-\theta)^{n-\sum_{i=1}^{n}x_i}$
		\item$g_{T_n}(t,\theta)={{n}\choose{t}}\sigma^t(1-\sigma)^{n-t}\mathbbm{1}_{0,1...n}(t)$
		\item$h_{\uX,T_n(z,t,\theta)}\p(\uX=\ux,T_n=t)=\sigma^t(1-\sigma)^{n-t}$
	\end{itemize}
	so 
	$$f_{\uX|T_n=t}=\frac{\sigma^t(1-\sigma)^{n-t}}{{{n}\choose{t}}\sigma^t(1-\sigma)^{n-t}}=\frac{1}{{{n}\choose{t}}}$$
	So $T_n$ is a sufficient statistic for $\theta$.\\
	This is a really special case because all the $X_i$ are already in function of $T_n$.
\end{eg}

\begin{oss}
	If $T_n$ is sufficient for $\theta$ then all the statistical information of $\theta$ is contained in the random sample relocated in $T_n$. In the example above we just need $\sum_{i=1}^{n}X_i$.\\
\end{oss}
\begin{oss}
	The notation of sufficiency derive from the probability structure of the parametric family $X\sim f_X(x;\theta)$ We can talk about sufficiency for a parameter $\theta$ only after we have specified $X\sim f_X(x;\theta)$
\end{oss}
The definition of sufficiency based on conditional probability is not of practical use because we need this two distributions $
\begin{cases}
g_{T_n}(\cdot)\\
h_{\uX,T_n}(\cdot,\cdot)
\end{cases}
$ that can be difficult to find.
To avoid that there is a corollary of the \textit{Fisher Factorization Theorem}:
\begin{corol}\label{corol:Savage}
	Let $\uX=(X_1... X_n)$ from $X\sim f_x(x,\theta)$. Then a statistic $T_n$ is sufficient for $\theta$ if and only if there exist two non negative functions $g(\cdot), h(\cdot)$ such that $\lf=g(T(\uX);\theta)h(\uX)$
\end{corol}
\begin{oss}
	\begin{itemize}
		\item $g$ is a function of the observed sample via $T_n$
		\item $h$ is a function of the observed sample and does not depend on $\theta$
 	\end{itemize}
\end{oss}
\begin{eg}
	Recall The example \ref{eg:ber} 
		$(X_1... X_n)\in \{0,1\}^n$ from a $Ber(\theta)$, $\theta \in (0,1)$.\\
	Define $T_n=\sum_{i=1}^{n}X_i$ then we want to verify if $T_n$ is sufficient.
	\begin{itemize}
	 \item $f_{\uX}(\ux;\theta) = \prod_{i=1}^{n} f_{\underline{X_i}}(\underline{x_i};\theta)=\theta^{\sum_{i=1}^{n}x_i}(1-\theta)^{n-\sum_{i=1}^{n}x_i}$\\
	\item $h(\uX)=1$
	\item$g(T_n(\uX),\theta)=\theta^{\sum_{i=1}^{n}x_i}(1-\theta)^{n-\sum_{i=1}^{n}x_i}$
	\end{itemize}
\end{eg}
\begin{eg}\label{eg:gauss}
		$(X_1... X_n)\in \{0,1\}^n$ from a $N(\theta,1)$. We want to verify that $T_n=\sum_{i=1}^{n} X_i$ is a sufficient statistic
		\[
		\begin{split}
			\lf
			&=\prod_{i=1}^{n}\frac{1}{\sqrt{2 \pi}} \exp \bigg\{ -\frac{1}{2} (x_i-\theta)^2 \bigg\}\\
			&=(2\pi)^{-n/2}\exp \bigg\{ -\frac{1}{2} \sum_{i=1}^{n}(x_i-\theta)^2 \bigg\}\\
			&=(2\pi)^{-n/2}\exp \bigg\{ -\frac{1}{2} \sum_{i=1}^{n}x_i^2- \frac{n\theta^2}{2}+\theta\sum_{i=1}^{n} x_i \bigg\}\\
			&=(2\pi)^{-n/2}\exp \bigg\{ -\frac{1}{2} \sum_{i=1}^{n}x_i^2 \bigg\} \exp \bigg\{- \frac{n\theta^2}{2}+\theta\sum_{i=1}^{n} x_i \bigg\}
		\end{split}
		\]
		so
		\begin{itemize}
			\item $h(x)=\exp \bigg\{ -\frac{1}{2} \sum_{i=1}^{n}x_i^2 \bigg\}$
			\item $g(\sum_{i=1}^n,\theta) = \exp \bigg\{- \frac{n\theta^2}{2}+\theta\sum_{i=1}^{n} x_i \bigg\}$
		\end{itemize}
\end{eg}

\begin{teo}\textbf{Fisher Theorem}\\
	If $\lfd$ is the joint density function or the joint probability mass function of $\uX$ and $q(t;\theta)$ is the density function or the probability mass function of $T_n(\uX)$, then $T_n(\uX)$ is sufficient for $\theta$ if for eery point in the sample space, the ratio
	$$\frac{\lfd}{q(t;\theta)}$$ 
	is a constant function of $\theta$
\end{teo}
\begin{proof}
	MISSING
\end{proof}
We can see now the prof of the corollary \ref{corol:Savage}
\begin{corol}\textbf{Savage}
	Let $\lfd$ be the jonit PDF or PMFof a random sample $\uX=(X_1... X_n)$. A statistic $T_n$ is sufficient for $\theta$ if and only if there exist two non negative functions $g(t,\theta), h(\ux)$ such that for all $\ux$ in the sample space and for all $\theta\in \Theta$ 
	$$\lfd=g(T(\ux);\theta)h(\ux)$$
\end{corol}
\begin{proof}
	We re going to prove that in the discrete settings.\\
	\begin{itemize}
		\item[$\Rightarrow$] Suppose that $T(\uX)$ is sufficient for $\theta$.\\
		\begin{itemize}
			\item $g(t,\theta)=\p(T(\uX)=t)$
			\item $h(\ux)=\p(\uX=\ux|T(\uX)=T(\ux)$
		\end{itemize}
	Because $T(\uX)$ is sufficient for $\theta$ the conditional probability defining $h(\ux)$ does not depend on $\theta$. Hence the choice of $g(t,\theta)$ and $h(\ux)$ is legitimate and for this choice we have
	\[
	\begin{split}
	\p(\uX=\ux)
	&=\p(\uX=\ux \wedge T(\uX)=T(\ux))\\
	&=\p(T(\uX)=T(\ux))\p(\uX=\ux|T(\uX)=T(\ux)\\
	&=g(t,\theta)h(\ux)
	\end{split}
	\]
	So we have the factorization and in particular we can see that
	$$\p(T(\uX)=T(\ux))=g(t,\theta)$$
	$\implies g(T(\ux),\theta)$ is the PMF of $T(s)$
	\item[$\Leftarrow$] We assume that the factorization holds.\\
	Let $q(t,\theta)$ be the PMF of $T(\uX)$ we study the ratio
	$$\frac{\lfd}{q(T(\ux);\theta)}$$
	in particular define
	$$A_{T(\ux)}=\{\underline y | T(\underline y)= T(\ux) \}$$
	Then 
	\[
	\begin{split}
	\frac{\lfd}{q(T(\ux);\theta)}
	&=\frac{g(T(\ux);\theta)h(\ux)}{q(T(\ux);\theta)}\\
	&=\frac{g(T(\ux);\theta)h(\ux)}{\sum_{\underline y \in A_{T(\ux)}}{g(T(\ux);\theta)h(\underline y)}}\\
	&=\frac{g(T(\ux);\theta)h(\ux)}{g(T(\ux);\theta) {\sum_{\underline y \in A_{T(\ux)}}h(\underline y)}}\\
	&=\frac{h(\ux)}{\sum_{\underline y \in A_{T(\ux)}}h(\underline y)}
	\end{split}
	\]
	This is constant \wrt $\theta$.\\
	Then by the Fisher Theorem $T(\uX)$ is sufficient for $\theta$.
	\end{itemize}

\end{proof}

\begin{eg}In with similar condition of the example \ref{eg:gauss} we want to find if $T_n=\frac{1}{n}\sum_{i=1}^{n} X_i$ is a sufficient statistic for $\mu$.
	$(X_1... X_n)\in \{0,1\}^n$ from a $N(\mu,\sigma^2)$, $\sigma^2$ known.
	\[
	\begin{split}
	f_{\uX}(\ux;\mu\sigma^2)
	&=(2 \pi \sigma^2)^{-n/2} \exp \bigg\{ -\frac{1}{2\sigma^2}\sum_{i=1}^{n} (x_i-\mu)^2 \bigg\}\\
	&=(2 \pi \sigma^2)^{-n/2} \exp \bigg\{ -\frac{1}{2\sigma^2} \sum_{i=1}^{n}(x_i-\bar x_n -\bar x_n -\mu)^2 \bigg\}\\
	&=(2 \pi \sigma^2)^{-n/2} \exp \bigg\{ -\frac{1}{2\sigma^2}\bigg( \sum_{i=1}^{n} (x_i-\bar x_n)^2 + n(\bar x_n-\mu)^2  \bigg) \bigg\}\\
	\end{split}
	\]
	we already have the distribution of $T(\ux)=\bar x_n=\frac{1}{n}\sum_{i=1}^{n}x_i$ which is $\bar X_n\sim N(\mu,\sigma^2/n)$.
	so we try o apply Fisher theorem to the ratio:
	\[
	\frac{(2 \pi \sigma^2)^{-n/2} \exp \bigg\{ -\frac{1}{2\sigma^2}\bigg( \sum_{i=1}^{n} (x_i-\bar x_n)^2 + n(\bar x_n-\mu)^2  \bigg) \bigg\}}{(2 \pi \sigma^2/n)^{-1/2}\exp \bigg\{- \frac{n}{2\sigma^2} (\bar x_n -\mu)^2 \bigg\}}
	\]
	So by Fisher Theorem $T_n=\frac{1}{n}\sum_{i=1}^{n} X_i$ is sufficient for $\mu$
\end{eg}
\begin{oss}
	Until now we found only one sufficient statistic for a fixed parametric model. However we can define many sufficient statistic.\\
	For example the statistic given by the identity $T(\ux)=\ux$ is always a sufficient statistic indeed we can factorize the distribution $f_X(x,\theta)$ with
	\begin{itemize}
		\item $h(x)=1$
		\item $g(T(x);\theta)=f_X(x,\theta)$
	\end{itemize}


Another  way to produce more sufficient statistics given one is thru a one to one function.\\
Suppose $T(\ux)$ is a sufficient statistic for $\theta$, and define $T*(\ux)=r(T(\ux))$ where $r$ is a one to one function with inverse $r^{-1}$.\\
By Savage's Theorem there exist $g,h$ such that
$$\lf=g(T(\ux),\theta)h(\ux)=g(r^{-1}T*(\ux),\theta)h(\ux)$$
So defining $g*(t\theta)=g(r^{-1}(t),\theta)$ we have that
$$\lf=g*(T*(\ux),\theta)h(\ux)$$
$\implies$ by Savage Theorem we have that $T*(\ux)$ is a sufficient statistic.
\end{oss}
We saw that in principle we can define many sufficient statistics so it is natural to define a tool that allows us to decide when a sufficient statistic is better than another.\\
Recall that the purpose of statistic is to achieve data reduction without loss of information.\\
Therefore a statistic that achieve the most data reduction while still retaining all of the information about $\theta$ might be preferable.
\begin{oss}
	We saw in example \ref{eg:gauss} that if 	$(X_1... X_n)\in \{0,1\}^n$ from a $N(\theta,1)$,  $T_n=\sum_{i=1}^{n} x_i$ is a sufficient statistic. Instead of $\sum_{i=1}^{n} X_i$ we can use $T'(\ux)\bigg( \sum_{i=1}^{n} x_i, \sum_{i=1}^{n} x_i^2 \bigg)$. Clearly $T(X)$ s a greater data reduction than $T'(\ux)$ since we do not need to know the sample variance if we want to know $T(\ux)$. Moreover we can write $T(\ux)$ as a function of $T'(\ux)$ by defining the function $r(a,b)=a$, then we can write
	$$T(\ux)=\bar x_n=r(\bar x_n, S^2_n)=r(T'(\ux))$$
	Since $T(\ux)$ and $T'(\ux)$ are both sufficient they contains the same information about $\mu$.
	In other terms the additional information given by the sample variance is null.
\end{oss} 
\begin{defi}
	A sufficient statistic $T(\ux)$ is called minimal if for any other sufficient statistic $T'(\ux)$, $T(\ux)$ is a function of $T'(\ux)$. 
\end{defi}
To say that $T(\ux)$  is a function of  $T'(\ux)$ simply means that if $T'(x)=T'(y)$ then $T(x)=T(y)$.\\
In other terms if $\{ B_t\} $ where $B_t:=\{t' : T'(t)=T'(t') \}$ is the partition set induced by $T'$ and $\{ A_t\} $ where $A_t:=\{t' : T(t)=T(t') \}$ is the partition set induced by $T$ then for every $t$, $B_t \subseteq A_t$.\\
$\implies$the partition of the sample space induced by a minimal statistic is the partition with the smallest cardinality.
\begin{teo}\textbf{Lehmann and Sheffe}
	let $\lfd$ be the joint density function or joint probability mass function of a \rs \  $\uX=(X_1...X_n)$. Suppose there exist a function $T$ such that for any two sample points $\ux$ , $\uy$ the ratio 
	\[
	\frac{\lfd}{f_{\uX}(\uy;\theta)}
	\]
	is constant as a function of $\theta$ if and only if $T(\ux)=T(\uy)$
	
	
	Then $T$is a minimal sufficient statistic for $\theta$ 
\end{teo}

\begin{proof}
	To simplify the proof we assume $\lfd > 0 \forall \ux, \forall \theta$.\\
	First we show that $T(\ux)$ is sufficient.\\
	Define $\Tau$ as the image of the sample space under the function $\st$.
	$$\Tau:=\{ t:t=\st \text{for some $\ux$ in the sample space} \}$$
	Define $\{ A_t\} $ where $A_t:=\{t' : T(t)=T(t') \}$, the partition set induced by $T$
	For each $A_t$  choose nd fix some elements $x_t\in A_t$. For any point in the space $\ux_{\st}$ is the fixed element that is in the same set ,$A_t$, as $\ux$. Since $\ux$ and $\ux_{\st}$ are in the same set $A_t$ then $\st=T(\ux_{\st})$ so by the assumptions the ratio
	\[
	\frac{\lfd}{f_{\uX}(\ux_{\st};\theta)}
	\]
Does not depend on $\theta$. Thus we can define $h(\ux):=\frac{\lfd}{f_{\uX}(\ux_{\st};\theta)}$.\\
Then define the function $g(\ux,\theta)\lfd$, so we have:
\[
\lfd =\frac{f_{\uX}(\ux_{\st};\theta)\lfd}{f_{\uX}(\ux_{\st};\theta)}=\gf h(\theta)
\]
and by Savage Theorem $\st$ is sufficient fo $\theta$.\\


Now we show that $\st$ is minimal sufficient.\\
Lt  $T'(\ux)$ be another sufficient statistic. By Savage Theorem we know that exist $h',g'$ such that
\[
\lfd=g'(T'(\ux),theta)h'(\theta)
\]
Let $\ux,\uy$ be two sample points such that $T'(\ux)=T'(\uy)$ then we can study the ratio:
\[
\frac{\lfd}{f_{\uX}(\uy;\theta)}=\frac{g'(T'(\ux);\theta)h'(\ux)}{g'(T'(\uy);\theta)h'(\uy)}=\frac{h'(\ux)}{h'(\uy)}
\]
Since the ratio does not depend on $\theta$, by the assumption (the other implication of the IIF) implies $\st =T(\uy)$. So we can say that $T(\ux)$ is a function of $T'(\ux)$ therefore $\st$ is minimal.
\end{proof}
\section{Estimators}

\begin{defi}
	Suppose there is a fixed parameter $\theta$ that needs to be estimated. Then an \textbf{estimator} is a function that maps the sample space to a set of sample estimates. An estimator of $\theta$ is usually denoted by the symbol $\bar \theta$.
\end{defi}
Now we re going to introduce some definition of \textit{"good"} estimators.
\begin{defi}
	$T_n(\uX)$ is said to be \textbf{unbiased} for $\theta$ if $\e[T_n(\uX)]=\theta$
\end{defi}
\begin{oss}
	In the definition we used the expected value because it is one of the linearity of the operator.
\end{oss}
When we ask an estimator to be unbiased basically we are requiring it to be centred around $\theta$.\\
Another parameter that give us information about the goodness of an estimator is the variance. We can interpret the variance as a measure of the dispersion around the expected value, so before check the variance we mus be sure that the expected value overlap with $\theta$. The less is the variance the best is the estimator.
\begin{oss}
	Variance is a good parameter to watch only if the estimator is unbiased
\end{oss}
To avoid this problem we can introduce the \textit{Mean Squared Error}
\begin{defi}\textbf{Mean Squared Error}
	\[
	\e[(T_n(\uX)-\theta)^2]
	\]
\end{defi}
The importance of this quantity comes from the \textit{Chebyshev's Inequality} \ref{eq:Chebyshev}
\[
\p(|T_n(\ux)-\theta|< k)>1-\frac{\e[(T_n(\uX)-\theta)^2]}{k^2}
\]






