\chapter{Introduction}
\label{cha:intro}
\vspace{15pt}




\section{Inequalities}
\label{sec:inequalities}
\vspace{10pt}
\textbf{Markow's Inequality}:\\
Let $Y$ be a non negative random variable with finite expected value then
$$ \mathbb{P}(Y \geq t)\leq \frac{\mathbb{E}[Y]}{t}$$
\textbf{Chebyshev's Inequality}:\\
Let $X$ be a random variable with finite second moment and let $\sigma=\sqrt{Var(x)}$, then for any positive real h
\begin{equation}\label{eq:Chebyshev}
	\mathbb{P}(|X-\mathbb{E}[X]|\geq h\sigma)\leq \frac{1}{h^2}
\end{equation}
\begin{teo}\textbf{Schwartz}
If $X,Y$ are random variables with finite second moment then:
$$ ( \mathbb{E} [XY])^2 \leq \e [X^2] \e [Y^2]$$
\end{teo}

If $X$ is a random variable taking values in a set $I$ with $\e [X]=\mu$  and $f()$ is convex on $I$, then $f(x)\geq f(\mu) + h(x-\mu)$ holds with probability 1 for some choice of h.
By integrating both sides of the inequality \wrt the distribution of X we obtain:
$$\e [f(x)]\geq f(\e [X]) \textbf{     (Jensen's Inequality)}$$

\section{Common distributions}
\label{sec:dist}
\textbf{Gaussian}:\\
A continuous random variable $Y$ is said to have a Gaussian distribution with parameters $\mu$ and $\sigma ^ 2$ if the density function at $t$ is:
$$\Gaus$$


Y is unimodal and symmetric around the mode $t=\mu$ and we write
$$Y \sim  N(\mu , \sigma ^2)$$
It's \textit{characteristic function} is
$$\e [e^{ tiy }]=\exp \Big\{ it \mu -\frac{\sigma ^2 t ^2}{2} \Big\}$$
The derivative of  the characteristic function valued in $t=0$ give us the non centred moments of Y.\\

If $Y\sim N(\mu \sigma ^2)$ and $a,b \in \mathbb{R}$ then $(a+bY) \sim N (a+b\mu, b^2 \sigma ^2)$. This mean that the entire family of distribution can be generated by linear transformations starting from any member of the family (i.e. is a \textit{location scale family})\\


If $Y_1 \sim N( \mu_1 , \sigma_1^2)$ and $Y_2 \sim N( \mu_2 , \sigma_2^2)$ and $ Y_1 \amalg Y_2$ then $Y_1+Y_2 \sim N (\mu _1 + \mu_2 , \sigma_1^2 + \sigma_2^2)$.\\
This result can be extended to linear combinations of Gaussian \rv s.\\
\\
\\
\\
\textbf{Uniform distribution:}\\
A continuous random variable can $Y$ with density function:
$$f(t;a,b)=\frac{1}{b-a} \mathbbm{1}_[a,b](t)$$
is said to be \textit{uniformly distributed} in $[a,b]$ and we write $Y\sim U(,b)$.\\
\begin{teo}\textbf{Integral transformation theorem}\\
If $Z$ is a continuous \rv \  with distribution function $F$ then the \rv 
$$W:=F(Z) \sim U(0,1)$$
\end{teo}
\begin{proof}
	\[
	\begin{split}
	\mathbb{P}(W\leq t)
	&=\p (F(Z)\leq t)\\
	&=\p (Z\leq F^{-1}(t))\\
	&=F(F^{-1}(t))\\
	&=t
	\end{split}
	\]
	Which is the distribution function of uniform in $[0,1]$
	
\end{proof}
\textbf{Gamma distribution:}\\
The \textit{Gamma function} is:
$$\Gamma (x):= \int_{0}^{+ \infty}t^{x-1}e^{-t}dt$$
Some properties of this function are:
\begin{itemize}
	\item $\Gamma (x+1)= x \Gamma(x)$
	\item if $x$ is a positive integer $\Gamma(x)=(x-1)!$
	\item $\Gamma(1)=1$
	\item $\Gamma \bigg( -\frac{1}{2} \bigg)=\sqrt{\pi}$
\end{itemize}
\textit{Stirling	's Approximation}
$$n!\sim \sqrt{2\pi n}\bigg(\frac{n}{e} \bigg)^n$$

\begin{defi}
	We say that a continuous \rv \  $X$ has a \textit{Gamma distribution} with shape parameter $w$ and scale parameter $\lambda$ ($X\sim Gamma(w,\lambda)$) if its density function is:
	$$f(t;w,\lambda)=\frac{\lambda^w}{\Gamma(w)}t^{w-1}e^{-\lambda t} \mathbbm{1}_{[\mathbb{R}^+]}(t)$$
\end{defi}

\begin{prop}
If $Y_1\sim Gamma(w_1,\lambda)$ and $Y_2\sim Gamma(w_2,\lambda)$ and $Y_1 \coprod Y_2$ then:
$$Y_1 + Y_2\sim Gamma(w_1+w_2,\lambda)$$
\end{prop}



Other distributions:
\begin{enumerate}
	\item Beta distribution
	\item Binomial Distribution
	\item Hypergeometric Distribution
	\item Negative Binomial distribution
\end{enumerate}


\section{Linear Algebra}
\textbf{Matrix}:\\
Consider $A,B$ two $n \times n$ squared matrix

\textit{Notation}:
\begin{enumerate}
	\item[$I_n$] is the identity matrix of order n
	\item[$1_n$] is the $n \times 1$ (column) vector with all elements equal to 1
	\item[$\bigcirc$] is a matrix with all element equal to zero
	\item[$|A|$] denotes the determinant of $A$
\end{enumerate}



\textit{Definitions / Properties}:

\begin{defi}
	$A$ is called \textit{symmetric matrix} if 
	$$A=A^T$$
\end{defi}


\begin{prop}
	for two conformable matrix $A,B$ we have:
	$$|AB|=|A||B|$$
\end{prop}


\begin{defi}
if $|A| \not = 0$, $A$ is called \textit{non singular} or \textit{invertible} and there exist a matrix $A^{-1}$ called \textit{inverse} such that 
$$AA^{-1}=A^{-1}A=I_n$$
\end{defi}

\begin{defi}
	A \textit{diagonal matrix} is a matrix with all elements outside the main diagonal equal to zero
\end{defi}

\begin{defi}
	A matrix $A$ is called \textit{invertible} if there exist an invertible matrix $P$ such that $P^{-1}AP$ is a diagonal matrix
\end{defi}
\begin{prop}
It holds:
$$(A^T)^{-1}=(A^{-1})^T$$
$$(AB)^{-1}=B^{-1}A^{-1}$$
\end{prop}


\begin{defi}
A symmetric matrix $A$ is said to be \textit{positive semi-definite} if 
$$v^TAv \geq0, \ \forall v\in \mathbb{R}^n$$
\end{defi}

\begin{defi}
A matrix $A$ is called orthogonal if:
$$A^{-1}=A^T$$
\end{defi}

\begin{defi}
Given a matrix $A$, we call \textit{trace of $A$} the sum of all the elements on the main diagonal:
$$Tr(A):= \sum_{i=1}^{n} a_{ii}$$
\end{defi}
\begin{prop}
For any matrix $A,B$ we have:
$$Tr(AB)=Tr(BA)$$
\end{prop}

\begin{defi}
An \textit{idempotent matrix} A is a matrix which, when multiplied by itself, yields itself i.e.:
$$AA=A$$
\end{defi}

\begin{prop}
	properties of an idempotent matrix A:
	\begin{enumerate}
		\item $I-M$ is also an idempotent matrix
		\item A is idempotent if and only of for all positive integers k, $A^k=A$
		\item an idempotent matrix is always diagonalizable and its eigenvalues are either 0 or 1
		\item the trace of an idempotent matrix is always an integer and it is equal to its rank
	\end{enumerate}
\end{prop}


\begin{prop}
	Two identities:
	\begin{enumerate}
		\item $(A+BCD)^{-1}=A^{-1} -A^{-1}B(C^{-1}+DA^{-1}B)^{-1}DA^{-1}$
		\item $(A+bd^T)^{-1}=A^{-1}-\frac{1}{1+d^TA^{-1}b}A^{-1}bd^TA^{-1}$
	\end{enumerate}
\end{prop}



\begin{teo}\textbf{Spectral theorem}\\
	Let $A$ be a symmetric $n \times n$ matrix, then there exist an orthogonal matrix $Q$ such that:
	$$A=Q \Lambda Q^T$$
	where $\Lambda$ is a diagonal matrix whose diagonal elements are the eigenvalues $\lambda_1....\lambda_n$ of $A$.
\end{teo}

\begin{corol}
$$|A|=|\Lambda|=\prod_{i=1}^n \lambda_i$$
\end{corol}


\section{Multivariate analysis}

\begin{defi}
Take $X_1....X_n$ random variables defined on the same probability space, we define the \textit{random vector} or the \textit{multivariate \rv} $X$ as:
$$X=(x_1....x_n)^T$$

\end{defi}
\begin{defi}
The \textit{mean vector} of $X$ is obtained by forming the vector of the mean values of the components
$$\e=(\e[X_1] \dots \e[x_n])^T$$
\end{defi}

\begin{defi}
	Similarly we can define the \textit{variance matrix} as:
	\[
	Var[X]:=\begin{bmatrix}
	Var(X_1) & Cov(X_1,X_2) & \dots  & Cov(X_1, X_{n}) \\
	Cov(X_2,X_1) & Var(X_2) &  \dots  & \vdots \\
	\vdots & \vdots  & \ddots & \vdots \\
	Cov(X_n,X_1) & \dots &  \dots  & Var(X_n)
	\end{bmatrix}	
	\]
\end{defi}

\begin{defi}
	A generic element of the \textit{correlation matrix} is defined as following:
	\[
	Corr(x_i,x_j) := \frac{Cov(X_i,X_j)}{\sqrt{Var(x_i)Var(X_j)}}	
	\]
\end{defi}

\begin{lem}
Let $A=a_{ij}$ be a $k \times n$ matrix, $b=(b_1 \dots b_n)^T$ a $n\times 1$ vector and  $x=(X_1\dots X_n)$ a random vector with $\mathbb{E}[x]=\mu$, $Var(x)=V$ define 
$$Y:=Ax+b$$
then
$$\mathbb{E}[Y]=A\mu+b$$
$$Var[Y]=AvA^T$$
\end{lem}

\begin{lem}
	The variance matrix $V$ of the random vector $X$ i positive semi-definite and it is positive definite if there exist no vectors $b$ such that $b^T$ is a degenerate \rv .
\end{lem}

\begin{lem}
If $A=(a_{ij})$ is a $n \times n$ matrix then:
$$\e [X^TAX]=\mu^T A \mu + Tr(AV)$$
\end{lem}
\begin{proof}
		\[
	\begin{split}
	\e  [X^TAX]
	&=\e[\sum_{i=1}^{n}\sum_{j=1}^{n}x_i a{ij}x_j]\\
	&=\sum_{i=1}^{n}\sum_{j=1}^{n} a_{ij}\e[x_i x_j]\\
	&=\sum_{i=1}^{n}\sum_{j=1}^{n}a_{ij}\mu_i\mu_j + v_{ij}\\
	&= \mu^T A \mu + \sum_{i=1}^{n}(AV)_{ii}\\
	&=\mu^T A \mu + Tr(AV)
	\end{split}
	\]
\end{proof}

\textbf{Multivariate Gaussian distribution:}\\

Consider a vector $Z=(Z_1...Z_k)^T$ where $Z_1....Z_k$ are \iid standard Gaussian \rv s. Now set 
$$Y=AZ+\mu$$
Where $A$ is a non singular  $k \times k$ matrix and $\mu$ is a $k \times 1$ vector.\\
It is natural to define $Y$ as a \textit{k-generated distribution of the Gaussian distribution}.\\
We start from:
$$f_z=\frac{1}{(2\pi)^{k/2}}\exp \bigg\{ -\frac{1}{2}t^Tt \bigg\}$$
($Z$ are independent so we simply multiplied them).\\
Since $Z=A^{-1}(Y-\mu)$, the Jacobian of the transformation is:
$$\bigg| \frac{dz_i}{dy_i} \bigg|=|A|^{-1}=|V|^{-1/2}$$
Taking into account that $|V|=|AA^T|=|A|^2$.

Setting $Y=At+\mu \implies t=A^{-1} (Y- \mu)$ we obtain:
$$t^Tt=\{ A^{-1} (Y- \mu) \}^T \{ A^{-1}(Y-\mu) \}=(Y-\mu)^T V^{-1}(Y-\mu)$$
Therefore the density of $Y$ is:
$$f_Y(y)=\frac{1}{(2\pi)^{k/2}|V|^{1/2}}\exp \bigg\{ -\frac{1}{2}(y-\mu)^T V^{-1} (y-\mu) \bigg\} $$
We say that the random variable $Y=(Y_1...Y_n)^T$ with density function $f_Y$ is a multivariate Gaussian \rv \ with mean $\mu$ and variance $V$. $Y\sim N_k(\mu,V)$.\\

Now we will explore \textit{marginal and conditional} distribution of Y.\\
\begin{prop}
	If $A$ is a $k\times k$  positive matrix and $b$ is a $k\times 1$ vector then:
	\[
	\int_{\mathbb{R}^k} \frac{1}{(2\pi)^k/2} \exp\bigg\{ - \frac{1}{2} (y^TAy - 2 b^T y )\bigg\} dy = \frac{\exp \{ 1/2 b^TA^{-1} b \} }{|A|^{1/2}}
\]
\end{prop}
\begin{proof}
	Let $\mu A^{-1}b$ and within the integral expand $\exp{}$ by adding and subtracting  $\frac{1}{2}\mu^TA\mu$ so that
	$$\int_{\mathbb{R}^k} \frac{1}{(2\pi)^k/2} \exp\bigg\{ - \frac{1}{2} (y^TAy - 2 b^T y )\bigg\} dy=|A^{-1}|^{1/2}\exp \bigg\{ \frac{1}{2} \mu^TA\mu \bigg\} \int_{\mathbb{R}^k}g(y)$$
\end{proof}

\section{Basic Concepts of Random Samples}

\begin{defi}
	Let $X_1...X_n$ \iid   \rv s with distribution $\sim f_{X_i}(x_i; \theta)$. We call $X:=(X_1...X_n)	$ \textit{random sample}
\end{defi}
According with the definition of \rs the distribution of $X$ will be:
$$f_X(X;\theta)= \prod_{i=1}^{n}f_{x_i}(x_i;\theta)$$
\begin{defi}
	We denote by $x=(x_1...x_n)$ the observed sample
\end{defi}

\begin{defi}
	A \textit{statistical model} is defined as following
	$$\{ f_{X}(x;\theta) : \theta \in \Theta \}$$
	Where $\Theta $ is the \textit{parametric space}
\end{defi}
Usually $\Theta$ will be a open subset of $\mathbb{R}^n$.\\


\begin{defi}
	Let $X_1 , ... , X_n$ be a random sample of size $n$ from a population and let $T(x_i, ... ,x_n)$ be a real-valued or vector-valued function whose domain includes
	the sample space of $(X_1, ... , X_n)$. Then the random variable or random vector
	$T_n = T(X_1, ... ,X_n)$ is called a \textit{statistic}. The probability distribution of a statistic $T_n$	is called the \textit{sampling distribution} of $T_n$.
\end{defi}
Some examples of statistic are:
\begin{itemize}
	\item Sample mean: $\bar X_n:=\frac{1}{n} \sum_{i=1}^n x_i$
	\item Sample variance: $\tilde S^2:=\frac{1}{n} \sum_{i=1}^n (x_i - \bar X )^2 $
	\item Corrected sample variance: $S^2:=\frac{1}{n-1} \sum_{i=1}^n (x_i - \bar X )^2 $
	\item Sample moments of order $r$: $M_{r,n}:=\frac{1}{n}\sum_{i=1}^{n}x_i^r$
	\item Sample moments of order $r$: $\bar M_{r,n}:=\frac{1}{n}\sum_{i=1}^{n}(x_i-\bar x_n)^r$
	\item Ordered statistic: $X_{(m)}$
	\item Sample min: $X_{(1)}$
	\item Sample max: $X_{(n)}$
	\item  
	$
	\text{Sample median:   }	Me:=\begin{cases}
	\frac{1}{2}(X_{(n/2)}+X_{(n/2+1)}) \ \ \ if \ n-even\\
	X_{\big(\frac{n+1}{2}\big)}\ \ \ \ \ \  if \ n-odd
	\end{cases}
	$
\end{itemize}
Finding the distribution of $T_n$ in general it is complex. We can make it easier by putting constrains.\\
Suppose for example $X\sim N(\mu, \sigma^2)\leftarrow$ fair assumption because there is the Central Limit Theorem.
\begin{teo}\textbf{Fisher Cochran}\\
	\label{teo:fishC}
	Let $Q,Q_1,Q_2$ \rv s such that $Q=Q_1+Q_2$ and let $Q\sim \mathcal{X}_{g}^2$ and $Q_2\sim\mathcal{X}_{g_1}^2$. Then
	$$Q_2\sim \mathcal{X}_{g_2}^2 \ \ \ \text{       where $g_2=g-g_1$,}$$
	and $Q_1 \coprod Q_2$
\end{teo}


\begin{prop}
	Let $ X_i\sim N(\mu, \sigma^2)$ and $X=(X_1...X_n)$. Then
	\begin{enumerate}
		\item $\bar X_n:=\frac{1}{n} \sum_{i=1}^n x_i \sim N(\mu, \frac{\sigma^2}{n})$
		\item $\tilde S_n:=\frac{1}{n} \sum_{i=1}^n (x_i - \bar X )^2 \sim \frac{\sigma^2}{n} \mathcal{X}_{n-1}^2$
	\end{enumerate}
	where $\mathcal{X}_{n-1}^2$ is the Chi-squared distribution with $n-1$ degrees of freedom
\end{prop}
\begin{proof}
	\begin{enumerate}
		\item the first one is easily checked using the linearity of the Gaussian distribution.
		\item Consider the \rv \  $\tilde S^2=\frac{1}{n}\sum_{i=1}^{n}(x_i- \mu)^2$ and proceed as following:
		\[
		\begin{split}
		\frac{n \tilde S^2}{\sigma^2}
		&=\sum_{i=1}^{n} \bigg( \frac{x_i- \mu}{\sigma} \bigg)^2\\
		&=\sum_{i=1}^{n} \bigg( \frac{x_i- \mu +\bar x_n -\bar x_n}{\sigma} \bigg)^2\\
		&=\sum_{i=1}^{n} \bigg( \frac{x_i- \bar x_n}{\sigma} \bigg)^2 + \sum_{i=1}^{n} \bigg( \frac{ \bar x_n - \mu}{\sigma} \bigg)^2 + 2 \sum_{i=1}^{n} \bigg( \frac{x_i- \bar x_n}{\sigma} \bigg) \bigg( \frac{ \bar x_n - \mu}{\sigma} \bigg)
		\end{split}
		\]
		Now consider separately the tree terms of the sum:\\
		$\sum_{i=1}^{n} \bigg( \frac{x_i- \bar x_n}{\sigma} \bigg) =n \bar x_n \sum_{i=1}^{n}x_i=n \bar x_n -n \bar x_n =0$\\
		$\implies 2 \sum_{i=1}^{n} \bigg( \frac{x_i- \bar x_n}{\sigma} \bigg) \bigg( \frac{ \bar x_n - \mu}{\sigma} \bigg)=0$ \\
		For the other two terms consider:\\
		$\sum_{i=1}^{n} \bigg( \frac{x_i- \mu}{\sigma} \bigg)^2 \sim \mathcal{X}_1^2$\\
		And\\
		$\sum_{i=1}^{n} \bigg( \frac{ \bar x_n - \mu}{\sigma} \bigg)^2= \bigg( \frac{ \bar x_n - \mu}{\sigma/ \sqrt{n}} \bigg)^2\sim \mathcal{X}_n^2$\\
		So, using the theorem \ref{teo:fishC}\\
		$$\sum_{i=1}^{n} \bigg( \frac{x_i- \mu}{\sigma} \bigg)^2= \sum_{i=1}^{n} \bigg( \frac{x_i- \bar x_n}{\sigma} \bigg)^2 +  \bigg( \frac{ \bar x_n - \mu}{\sigma/ \sqrt{n}} \bigg)^2 \sim \mathcal{X}_{n-1}^2$$
		$\implies \tilde S_n^2\sim \frac{\sigma^2}{n} \mathcal{X}_{n-1}^2 $
	\end{enumerate}
\end{proof}

Let $X=(X_1...X_n)$ be a random sample where $X_i\sim N(\mu,\sigma^2)$, consider the statistic:
$$T_n=\frac{\bar X_n-\mu}{ S_n/\sqrt{n}}=\frac{(\bar X_n-\mu)/\sigma/\sqrt{n}}{\sqrt{S_n/\sigma^2}}=\frac{Z}{\sqrt{R/(n-1)}}$$
where $Z\sim N(0,1), R\sim(\mathcal{X}_n-1)$
We can say that $T_n\sim T-Student$ with $(n-1)$ degrees of freedom only if $Z \coprod R$. We can they are independent because of the following:
\begin{teo}
	If $(x_1...x_n)$ random sample with $X_i\sim N(\mu \sigma^2)$ then 
	$$\bar X_n =\frac{1}{n} \sum_{i=1}^{n} x_i$$	
	$$S^2=\frac{1}{n-1} \sum_{i=1}^{n} (x_i -\bar X_n)^2$$
	are independent
\end{teo}
The other way around is also true:
\begin{teo}
	If $\bar X_n =\frac{1}{n} \sum_{i=1}^{n} x_i$, $S^2=\frac{1}{n-1}\sum_{i=1}^{n} (x_i -\bar X_n)^2$ are independent then $X=(x_1...x_n)$ is random sample where $X_i\sim N(\mu \sigma^2)$ 
\end{teo}

